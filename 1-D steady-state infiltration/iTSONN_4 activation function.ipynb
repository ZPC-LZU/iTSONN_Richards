{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d813de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)  # Set the random seed to 42\n",
    "\n",
    "# Set the font for matplotlib to support Chinese characters\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Function to transform predicted and true values\n",
    "def transform_values(psi):\n",
    "    return (1 / 0.008) * torch.log(psi + torch.exp(torch.tensor(-4.0)))\n",
    "\n",
    "# Forward gradient function\n",
    "def fwd_gradients(Y, x):\n",
    "    dummy = torch.ones_like(Y)\n",
    "    G = torch.autograd.grad(Y, x, dummy, create_graph=True)[0]\n",
    "    return G\n",
    "\n",
    "# Neural network class with Xavier initialization and weight normalization\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, layer_dim, X, device):\n",
    "        super().__init__()\n",
    "        self.X_mean = torch.from_numpy(X.mean(0, keepdims=True)).float().to(device)\n",
    "        self.X_std = torch.from_numpy(X.std(0, keepdims=True)).float().to(device)\n",
    "\n",
    "        self.num_layers = len(layer_dim)\n",
    "        temp = []\n",
    "        for l in range(1, self.num_layers):\n",
    "            linear = weight_norm(torch.nn.Linear(layer_dim[l-1], layer_dim[l]), dim=0)\n",
    "            torch.nn.init.xavier_uniform_(linear.weight)\n",
    "            if linear.bias is not None:\n",
    "                torch.nn.init.zeros_(linear.bias)\n",
    "            temp.append(linear)\n",
    "        self.layers = torch.nn.ModuleList(temp)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - self.X_mean) / self.X_std  # Normalize input\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            x = self.layers[i](x)\n",
    "            if i < self.num_layers-2:\n",
    "                x = F.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# TSONN class for training and model management\n",
    "class TSONN:\n",
    "    def __init__(self, layers, device):\n",
    "        self.layers = layers\n",
    "        self.device = device\n",
    "\n",
    "        Nx = 101  # Number of grid points\n",
    "        x = torch.linspace(0, 10, Nx).to(self.device)\n",
    "        self.X_ref = x.reshape(-1, 1)\n",
    "\n",
    "        # Boundary conditions at x = 0 and x = 10\n",
    "        self.X_lbc = torch.tensor([[0]], device=self.device)\n",
    "        self.X_ubc = torch.tensor([[10.0]], device=self.device)\n",
    "\n",
    "        self.Nx = Nx\n",
    "        self.log = {'losses': [], 'losses_b': [], 'losses_f': [], 'time': []}\n",
    "        self.min_loss = 1\n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = Net(self.layers, self.X_ref.cpu().detach().numpy(), self.device).to(self.device)\n",
    "\n",
    "    def Mseb(self):\n",
    "        \"\"\"Compute the boundary condition loss\"\"\"\n",
    "        pred_lbc = self.model(self.X_lbc)\n",
    "        pred_ubc = self.model(self.X_ubc)\n",
    "        mseb = ((pred_lbc - 0) ** 2 + (pred_ubc - (1 - torch.exp(torch.tensor(-4.0)))) ** 2).mean()\n",
    "        return mseb\n",
    "\n",
    "    def TimeStepping(self):\n",
    "        \"\"\"Time stepping update\"\"\"\n",
    "        X = self.X\n",
    "        pred = self.model(X)\n",
    "        u = pred\n",
    "        self.U0 = u.detach()\n",
    "\n",
    "    def Msef(self):\n",
    "        \"\"\"Compute the PDE residual loss\"\"\"\n",
    "        X = self.X\n",
    "        pred = self.model(X)\n",
    "        u = pred\n",
    "        u_x = fwd_gradients(u, X)\n",
    "        u_xx = fwd_gradients(u_x, X)\n",
    "        a = 0.008\n",
    "        res = u_x * a + u_xx\n",
    "        U1 = u\n",
    "        R1 = res\n",
    "\n",
    "        dtau = 100  # Time step size\n",
    "        msef = 1 / dtau ** 2 * ((U1 - self.U0 + dtau * R1) ** 2).mean()\n",
    "        return msef\n",
    "\n",
    "    def Loss(self):\n",
    "        \"\"\"Compute the total loss\"\"\"\n",
    "        mseb = self.Mseb()\n",
    "        msef = self.Msef()\n",
    "        loss = mseb + msef\n",
    "        return loss, mseb, msef\n",
    "\n",
    "    def ResidualPoint(self):\n",
    "        \"\"\"Generate PDE residual points\"\"\"\n",
    "        self.X = torch.linspace(0, 10, 2000, device=self.device).reshape(-1, 1)\n",
    "        self.X.requires_grad = True\n",
    "\n",
    "    def train(self, epoch):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        if len(self.log['time']) == 0:\n",
    "            t1 = time.time()\n",
    "        else:\n",
    "            t1 = time.time() - self.log['time'][-1]\n",
    "\n",
    "        # Adam optimizer with learning rate\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=3e-4)\n",
    "\n",
    "        for i in range(epoch):\n",
    "            self.ResidualPoint()\n",
    "            self.TimeStepping()\n",
    "\n",
    "            # Forward pass and loss calculation\n",
    "            self.optimizer.zero_grad()\n",
    "            self.loss, self.loss_b, self.loss_f = self.Loss()\n",
    "            self.loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Record loss and time\n",
    "            t2 = time.time()\n",
    "            self.log['losses'].append(self.loss.item())\n",
    "            self.log['losses_b'].append(self.loss_b.item())\n",
    "            self.log['losses_f'].append(self.loss_f.item())\n",
    "            self.log['time'].append(t2 - t1)\n",
    "\n",
    "            # Print training progress\n",
    "            print(f'Epoch {i + 1}/{epoch} - Total Loss: {self.loss.item():.6f}, Boundary Loss: {self.loss_b.item():.6f}, PDE Loss: {self.loss_f.item():.6f}')\n",
    "            if (i % np.clip(int(epoch / 1000), 1, 1000) == 0) or (i == epoch - 1):\n",
    "                print(f'{i}|{epoch} Total Loss={self.loss.item():.6f} PDE Loss={self.loss_f.item():.6f}')\n",
    "\n",
    "# Exact solution function\n",
    "def func(x):\n",
    "    a = 0.008\n",
    "    phi_d = -500\n",
    "    x_cpu = x.detach().cpu().numpy()\n",
    "    term1 = (1 - np.exp(a * phi_d)) * np.exp(a / 2 * (10 - x_cpu))\n",
    "    sinh_ratio = np.sinh(a * x_cpu / 2) / np.sinh(a * 10 / 2)\n",
    "    term2 = np.exp(a * phi_d)\n",
    "    return torch.tensor(term1 * sinh_ratio, dtype=torch.float32)\n",
    "\n",
    "# Main program\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    layers = [1, 128, 128, 128, 1]\n",
    "    tsonn = TSONN(layers, device)\n",
    "    tsonn.train(10000)  # Set 10000 iterations\n",
    "\n",
    "    # Save total loss to Excel file\n",
    "    loss_df = pd.DataFrame({\n",
    "        'Epoch': list(range(1, len(tsonn.log['losses']) + 1)),\n",
    "        'Total Loss': tsonn.log['losses']\n",
    "    })\n",
    "    loss_df.to_excel('total_loss_silu.xlsx', index=False)\n",
    "    print(\"Total loss values saved to total_loss.xlsx\")\n",
    "\n",
    "    # Print total loss statistics\n",
    "    print(\"\\n=== Total Loss Statistics ===\")\n",
    "    print(f\"Total training epochs: {len(tsonn.log['losses'])}\")\n",
    "    print(f\"Initial total loss: {tsonn.log['losses'][0]:.6f}\")\n",
    "    print(f\"Final total loss: {tsonn.log['losses'][-1]:.6f}\")\n",
    "    print(f\"Minimum total loss: {min(tsonn.log['losses']):.6f}\")\n",
    "    print(f\"Average total loss: {np.mean(tsonn.log['losses']):.6f}\")\n",
    "\n",
    "    # Plot loss curve\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(tsonn.log['losses'], label='Total Loss', color='blue')\n",
    "    plt.plot(tsonn.log['losses_b'], label='Boundary Loss', color='orange')\n",
    "    plt.plot(tsonn.log['losses_f'], label='PDE Loss', color='green')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss Value (log scale)')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    plt.annotate(f'Final Total Loss: {tsonn.log[\"losses\"][-1]:.2e}',\n",
    "                 xy=(len(tsonn.log['losses']), tsonn.log['losses'][-1]),\n",
    "                 xytext=(len(tsonn.log['losses'])-2000, tsonn.log['losses'][-1]*2),\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "    plt.savefig('loss_curve_annotated_tanh.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Predict values at 101 points in the range [0, 10]\n",
    "    x = torch.linspace(0, 10, 101).reshape(-1, 1).to(device)\n",
    "    u_pred = tsonn.model(x).cpu().detach().numpy()\n",
    "    u_exact = func(x.cpu()).detach().numpy()\n",
    "\n",
    "    # Transform values - Add NaN check and handling\n",
    "    u_pred_transformed = transform_values(torch.tensor(u_pred)).cpu().detach().numpy()\n",
    "    u_exact_transformed = transform_values(torch.tensor(u_exact)).cpu().detach().numpy()\n",
    "\n",
    "    # Check and handle NaN values\n",
    "    mask = ~np.isnan(u_pred_transformed.flatten()) & ~np.isnan(u_exact_transformed.flatten())\n",
    "    u_pred_transformed = u_pred_transformed.flatten()[mask]\n",
    "    u_exact_transformed = u_exact_transformed.flatten()[mask]\n",
    "    x_filtered = x.cpu().numpy().flatten()[mask]\n",
    "\n",
    "    if len(u_pred_transformed) == 0:\n",
    "        raise ValueError(\"All transformed values are NaN, please check the transform function and model output\")\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    r2_transformed = r2_score(u_exact_transformed, u_pred_transformed)\n",
    "    relative_l2_error_transformed = np.linalg.norm(u_exact_transformed - u_pred_transformed) / np.linalg.norm(u_exact_transformed)\n",
    "\n",
    "    print(f\"R² Score (transformed): {r2_transformed:.4f}\")\n",
    "    print(f\"Relative L2 Error (transformed): {relative_l2_error_transformed:.4e}\")\n",
    "\n",
    "    # Save prediction results\n",
    "    data = {\n",
    "        'x': x_filtered,\n",
    "        'True Value': u_exact.flatten()[mask],\n",
    "        'Predicted Value': u_pred.flatten()[mask],\n",
    "        'Transformed True Value': u_exact_transformed,\n",
    "        'Transformed Predicted Value': u_pred_transformed\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel('predictions_101_points_silu.xlsx', index=False)\n",
    "    print(\"Prediction results saved to predictions_101_points.xlsx\")\n",
    "\n",
    "    # Plot comparison of solutions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x_filtered, u_pred_transformed, label='Transformed Predicted u(x)')\n",
    "    plt.plot(x_filtered, u_exact_transformed, label='Transformed Exact u(x)', linestyle='dashed')\n",
    "    plt.title('Comparison of Transformed Predicted and Exact Solutions (Valid Points)')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Transformed u')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('solution_comparison.png')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
