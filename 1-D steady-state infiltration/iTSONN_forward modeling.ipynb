{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb50d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.utils.parametrizations import weight_norm  # Updated weight normalization\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Set matplotlib font to support Chinese characters\n",
    "plt.rcParams['font.family'] = 'SimHei'  # Set font to SimHei for Chinese characters\n",
    "plt.rcParams['axes.unicode_minus'] = False  # Ensure negative signs are displayed correctly\n",
    "\n",
    "# -----------------------------\n",
    "# Function to transform predicted and true values\n",
    "def transform_values(psi):\n",
    "    return (1 / 0.008) * torch.log(psi + torch.exp(torch.tensor(-8.0)))\n",
    "\n",
    "# Forward gradient function (kept unchanged)\n",
    "def fwd_gradients(Y, x):\n",
    "    dummy = torch.ones_like(Y)\n",
    "    G = torch.autograd.grad(Y, x, dummy, create_graph=True)[0]\n",
    "    return G \n",
    "\n",
    "# Neural Network class with Xavier initialization and weight normalization\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, layer_dim, X, device):\n",
    "        super().__init__()\n",
    "        # Normalization parameters\n",
    "        self.X_mean = torch.from_numpy(X.mean(0, keepdims=True)).float().to(device)\n",
    "        self.X_std = torch.from_numpy(X.std(0, keepdims=True)).float().to(device)\n",
    "\n",
    "        self.num_layers = len(layer_dim)\n",
    "        temp = []\n",
    "        for l in range(1, self.num_layers):\n",
    "            linear = weight_norm(torch.nn.Linear(layer_dim[l-1], layer_dim[l]), dim=0)\n",
    "            torch.nn.init.xavier_uniform_(linear.weight)\n",
    "            if linear.bias is not None:\n",
    "                torch.nn.init.zeros_(linear.bias)\n",
    "            temp.append(linear)\n",
    "        self.layers = torch.nn.ModuleList(temp)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - self.X_mean) / self.X_std  # Normalize the input\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            x = self.layers[i](x)\n",
    "            if i < self.num_layers-2:\n",
    "                x = F.silu(x)  # Use the SiLU activation function\n",
    "        return x\n",
    "\n",
    "# TSONN class for training and model management\n",
    "class TSONN():\n",
    "    def __init__(self, layers, device):\n",
    "        self.layers = layers\n",
    "        self.device = device\n",
    "\n",
    "        Nx = 101  # Number of grid points\n",
    "        x = torch.linspace(0, 10, Nx).to(self.device)\n",
    "        self.X_ref = x.reshape(-1, 1)\n",
    "\n",
    "        # Boundary conditions at x = 0 and x = 10\n",
    "        self.X_lbc = torch.tensor([[0]], device=self.device)\n",
    "        self.X_ubc = torch.tensor([[10.0]], device=self.device)\n",
    "\n",
    "        self.Nx = Nx\n",
    "        self.log = {'losses': [], 'losses_b': [], 'losses_f': [], 'time': []}\n",
    "        self.min_loss = 1\n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = Net(self.layers, self.X_ref.cpu().detach().numpy(), self.device).to(self.device)\n",
    "\n",
    "    def Mseb(self):\n",
    "        \"\"\"Compute the boundary condition loss\"\"\"\n",
    "        pred_lbc = self.model(self.X_lbc)\n",
    "        pred_ubc = self.model(self.X_ubc)\n",
    "        mseb = ((pred_lbc - 0) ** 2 + (pred_ubc - (1 - torch.exp(torch.tensor(-8.0)))) ** 2).mean()\n",
    "        return mseb\n",
    "\n",
    "    def TimeStepping(self):\n",
    "        \"\"\"Time stepping update\"\"\"\n",
    "        X = self.X\n",
    "        pred = self.model(X)\n",
    "        u = pred\n",
    "        self.U0 = u.detach()\n",
    "\n",
    "    def Msef(self):\n",
    "        \"\"\"Compute the PDE residual loss\"\"\"\n",
    "        X = self.X\n",
    "        pred = self.model(X)\n",
    "        u = pred\n",
    "        u_x = fwd_gradients(u, X)\n",
    "        u_xx = fwd_gradients(u_x, X)\n",
    "        a = 0.008\n",
    "        res = u_x * a + u_xx\n",
    "        U1 = u\n",
    "        R1 = res\n",
    "\n",
    "        dtau = 10  # Time step size\n",
    "        msef = 1 / dtau ** 2 * ((U1 - self.U0 + dtau * R1) ** 2).mean()\n",
    "        return msef\n",
    "\n",
    "    def Loss(self):\n",
    "        \"\"\"Compute the total loss\"\"\"\n",
    "        mseb = self.Mseb()\n",
    "        msef = self.Msef()\n",
    "        loss = mseb + msef\n",
    "        return loss, mseb, msef\n",
    "\n",
    "    def ResidualPoint(self):\n",
    "        \"\"\"Generate PDE residual points\"\"\"\n",
    "        self.X = torch.linspace(0, 10, 200, device=self.device).reshape(-1, 1)\n",
    "        self.X.requires_grad = True\n",
    "\n",
    "    def train(self, epoch):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        if len(self.log['time']) == 0:\n",
    "            t1 = time.time()\n",
    "        else:\n",
    "            t1 = time.time() - self.log['time'][-1]\n",
    "\n",
    "        for i in range(epoch):\n",
    "            def closure():\n",
    "                self.optimizer.zero_grad()\n",
    "                self.loss, self.loss_b, self.loss_f = self.Loss()\n",
    "                self.loss.backward()\n",
    "                return self.loss\n",
    "\n",
    "            self.optimizer = torch.optim.LBFGS(self.model.parameters(), max_iter=100)\n",
    "            self.ResidualPoint()\n",
    "            self.TimeStepping()\n",
    "            self.optimizer.step(closure)\n",
    "\n",
    "            if (self.loss != self.loss) or ((i > 1) and (self.loss.item() > 3 * self.log['losses'][-1])):  # Check for NaN or exploding loss\n",
    "                if i == 0:\n",
    "                    self.model = Net(self.layers, self.X_ref.cpu().detach().numpy(), self.device).to(self.device)\n",
    "                    continue\n",
    "                else:\n",
    "                    self.model.load_state_dict(torch.load('model_temp.pth'))  # Load model from checkpoint\n",
    "                    print('Model loaded from checkpoint')\n",
    "                    self.ResidualPoint()\n",
    "                    self.optimizer = torch.optim.LBFGS(self.model.parameters(), max_iter=100)\n",
    "                    continue\n",
    "\n",
    "            if i % 3 == 0:\n",
    "                torch.save(self.model.state_dict(), 'model_temp.pth')  # Save model every 3 iterations\n",
    "\n",
    "            t2 = time.time()\n",
    "            self.log['losses'].append(self.loss.item())\n",
    "            self.log['losses_b'].append(self.loss_b.item())\n",
    "            self.log['losses_f'].append(self.loss_f.item())\n",
    "            self.log['time'].append(t2 - t1)\n",
    "\n",
    "            print(f'Epoch {i + 1}/{epoch} - Total Loss: {self.loss.item()}, Boundary Loss: {self.loss_b.item()}, PDE Loss: {self.loss_f.item()}')\n",
    "            if (i % np.clip(int(epoch / 1000), 1, 1000) == 0) or (i == epoch - 1):\n",
    "                print(f'{i}|{epoch} Total Loss={self.loss.item()} PDE Loss={self.loss_f.item()}')\n",
    "\n",
    "# Exact solution function\n",
    "def func(x):\n",
    "    a = 0.008\n",
    "    phi_d = -1000\n",
    "    x_cpu = x.detach().cpu().numpy()\n",
    "    term1 = (1 - np.exp(a * phi_d)) * np.exp(a / 2 * (10 - x_cpu))\n",
    "    sinh_ratio = np.sinh(a * x_cpu / 2) / np.sinh(a * 10 / 2)\n",
    "    term2 = np.exp(a * phi_d)\n",
    "    return torch.tensor(term1 * sinh_ratio, dtype=torch.float32)\n",
    "\n",
    "# -----------------------------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    layers = [1, 128, 128, 128,  1]\n",
    "    tsonn = TSONN(layers, device)\n",
    "    tsonn.train(100)\n",
    "\n",
    "    x = tsonn.X_ref.cpu().detach().numpy()\n",
    "    u_pred = tsonn.model(tsonn.X_ref).cpu().detach().numpy()\n",
    "    u_exact = func(torch.tensor(x, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "    u_pred_transformed = transform_values(torch.tensor(u_pred)).cpu().detach().numpy()\n",
    "    u_exact_transformed = transform_values(torch.tensor(u_exact)).cpu().detach().numpy()\n",
    "    u_pred_transformed = u_pred_transformed.flatten()\n",
    "    u_exact_transformed = u_exact_transformed.flatten()\n",
    "\n",
    "    r2_transformed = r2_score(u_exact_transformed, u_pred_transformed)\n",
    "    relative_l2_error_transformed = np.linalg.norm(u_exact_transformed - u_pred_transformed) / np.linalg.norm(u_exact_transformed)\n",
    "\n",
    "    print(f\"RÂ² Score (transformed): {r2_transformed:.4f}\")\n",
    "    print(f\"Relative L2 Error (transformed): {relative_l2_error_transformed:.4e}\")\n",
    "\n",
    "    data = {\n",
    "        'z': x.flatten(),\n",
    "        'Transformed True Values': u_exact_transformed,\n",
    "        'Transformed Predicted Values': u_pred_transformed\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel('transformed_predictions.xlsx', index=False)\n",
    "\n",
    "    # Plot the comparison of solutions\n",
    "    plt.plot(x, u_pred_transformed, label='Transformed Predicted u(x)')\n",
    "    plt.plot(x, u_exact_transformed, label='Transformed Exact u(x)', linestyle='dashed')\n",
    "    plt.title('Comparison of Transformed Predicted and Exact Solutions')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Transformed u')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
